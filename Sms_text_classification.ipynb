{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8RZOuS9LWQvv",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  !pip install tf-nightly\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "!pip install tensorflow-datasets\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMHwYXHXCar3",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# get data files\n",
        "!wget https://cdn.freecodecamp.org/project-data/sms/train-data.tsv\n",
        "!wget https://cdn.freecodecamp.org/project-data/sms/valid-data.tsv\n",
        "\n",
        "train_file_path = \"train-data.tsv\"\n",
        "test_file_path = \"valid-data.tsv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_h508FEClxO"
      },
      "outputs": [],
      "source": [
        "# Data Loading and Initial Inspection\n",
        "\n",
        "# Load the training data from the .tsv file into a pandas DataFrame\n",
        "# .tsv files use tab as a separator ('\\t')\n",
        "# The data appears to have two columns: label (ham/spam) and message text.\n",
        "# We can use read_csv and specify the separator.\n",
        "train_df = pd.read_csv(train_file_path, sep='\\t', header=None, names=['label', 'message'])\n",
        "\n",
        "# Load the validation (test) data similarly\n",
        "test_df = pd.read_csv(test_file_path, sep='\\t', header=None, names=['label', 'message'])\n",
        "\n",
        "print(\"Train DataFrame Head:\")\n",
        "print(train_df.head())\n",
        "print(\"\\nTrain DataFrame Info:\")\n",
        "print(train_df.info())\n",
        "print(\"\\nTrain DataFrame Shape:\", train_df.shape)\n",
        "\n",
        "print(\"\\nTest DataFrame Head:\")\n",
        "print(test_df.head())\n",
        "print(\"\\nTest DataFrame Info:\")\n",
        "print(test_df.info())\n",
        "print(\"\\nTest DataFrame Shape:\", test_df.shape)\n",
        "\n",
        "# Check the distribution of labels in the training data\n",
        "print(\"\\nLabel distribution in training data:\")\n",
        "print(train_df['label'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOMKywn4zReN"
      },
      "outputs": [],
      "source": [
        "# Text Preprocessing and Label Encoding\n",
        "\n",
        "# Import necessary libraries for text processing\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# --- 1. Convert Labels to Numerical ---\n",
        "# Initialize LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit the encoder on the combined labels from train and test data\n",
        "# This ensures consistent mapping if a label appears only in one set\n",
        "# Although in this case, both 'ham' and 'spam' are likely in both sets.\n",
        "# We fit on the training labels and then transform both train and test labels.\n",
        "train_labels_encoded = label_encoder.fit_transform(train_df['label'])\n",
        "test_labels_encoded = label_encoder.transform(test_df['label'])\n",
        "\n",
        "# Check the mapping (optional)\n",
        "print(\"Label Mapping:\")\n",
        "print(dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))))\n",
        "# Output should show something like {'ham': 0, 'spam': 1} or vice versa.\n",
        "# Let's assume 'ham' is 0 and 'spam' is 1 based on common practice and the test case.\n",
        "\n",
        "# --- 2. Convert Text Messages to Numerical Feature Vectors (TF-IDF) ---\n",
        "# Initialize TfidfVectorizer\n",
        "# max_features: Limits the number of features (words) to consider.\n",
        "#               This helps manage dimensionality and focus on most frequent words.\n",
        "#               Adjust this number based on dataset size and desired complexity.\n",
        "#               Let's start with 5000 features.\n",
        "# stop_words='english': Removes common English words (like 'the', 'is', 'in')\n",
        "#                       that usually don't carry much meaning for classification.\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
        "\n",
        "# Fit the vectorizer ONLY on the training message text\n",
        "# The vectorizer learns the vocabulary and IDF values from the training data.\n",
        "tfidf_vectorizer.fit(train_df['message'])\n",
        "\n",
        "# Transform both training and test message text into TF-IDF vectors\n",
        "# We use the SAME vectorizer fitted on the training data to transform the test data.\n",
        "train_messages_tfidf = tfidf_vectorizer.transform(train_df['message'])\n",
        "test_messages_tfidf = tfidf_vectorizer.transform(test_df['message'])\n",
        "\n",
        "print(\"\\nShape of TF-IDF transformed training messages:\", train_messages_tfidf.shape)\n",
        "print(\"Shape of TF-IDF transformed test messages:\", test_messages_tfidf.shape)\n",
        "\n",
        "# Now we have our numerical data ready for the model:\n",
        "# X_train = train_messages_tfidf (Sparse matrix of TF-IDF features)\n",
        "# X_test = test_messages_tfidf (Sparse matrix of TF-IDF features)\n",
        "# y_train = train_labels_encoded (Numpy array of 0s and 1s)\n",
        "# y_test = test_labels_encoded (Numpy array of 0s and 1s)\n",
        "\n",
        "# We'll use these variables in the next steps.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Definition, Compilation, and Training\n",
        "\n",
        "# Ensure the 'layers' module is imported\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential # Also ensure Sequential is imported if not globally\n",
        "from tensorflow.keras.optimizers import Adam # Ensure Adam is imported if not globally\n",
        "\n",
        "# Define the model architecture\n",
        "# We use a Sequential model.\n",
        "model = keras.Sequential([\n",
        "    # The input layer needs to match the number of features from TF-IDF.\n",
        "    # train_messages_tfidf is a sparse matrix, its shape is (num_samples, num_features).\n",
        "    # We need the number of features (columns).\n",
        "    # The input shape for the first Dense layer is the number of TF-IDF features.\n",
        "    layers.Dense(128, activation='relu', input_shape=(train_messages_tfidf.shape[1],)),\n",
        "    # Add another Dense layer\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    # The output layer for binary classification (ham/spam) has 1 unit.\n",
        "    # We use 'sigmoid' activation to output a probability between 0 and 1.\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "# Optimizer: Adam is a good default choice.\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001) # Use tf.keras.optimizers.Adam\n",
        "\n",
        "# Loss function: For binary classification, binary_crossentropy is standard.\n",
        "loss_function = 'binary_crossentropy'\n",
        "\n",
        "# Metrics: We want to monitor accuracy.\n",
        "metrics_to_monitor = ['accuracy']\n",
        "\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=loss_function,\n",
        "              metrics=metrics_to_monitor)\n",
        "\n",
        "# Print the model summary\n",
        "print(\"Model Summary:\")\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "# We use the .fit() method.\n",
        "# train_messages_tfidf: The TF-IDF features for training.\n",
        "# train_labels_encoded: The numerical labels (0s and 1s) for training.\n",
        "# epochs: Number of times to iterate over the training data. Text data often needs fewer epochs than images.\n",
        "#         Start with a moderate number, maybe 10-20.\n",
        "# batch_size: Number of samples per gradient update.\n",
        "# verbose: Show training progress (1 for progress bar).\n",
        "# validation_data: Pass the test set here to monitor performance during training (optional but good practice).\n",
        "\n",
        "print(\"\\nStarting model training...\")\n",
        "\n",
        "history = model.fit(\n",
        "    train_messages_tfidf,\n",
        "    train_labels_encoded,\n",
        "    epochs=15, # Adjust epochs as needed\n",
        "    batch_size=32, # Adjust batch size as needed\n",
        "    verbose=1, # Set to 0 for silent, 2 for one line per epoch\n",
        "    validation_data=(test_messages_tfidf, test_labels_encoded) # Monitor performance on test set\n",
        ")\n",
        "\n",
        "print(\"Model training finished.\")\n",
        "\n",
        "# The model is now trained and ready to be used for predictions.\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Zhw04yy4z7g2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to predict messages based on model\n",
        "# (should return list containing prediction probability and label, ex. [0.008318834938108921, 'ham'])\n",
        "\n",
        "# Ensure the necessary objects from previous steps are available:\n",
        "# tfidf_vectorizer (fitted)\n",
        "# model (trained)\n",
        "# label_encoder (fitted, to map back from 0/1 to 'ham'/'spam')\n",
        "\n",
        "def predict_message(pred_text):\n",
        "    # --- Step 1: Preprocess the input message ---\n",
        "    # The model was trained on TF-IDF vectors, so the input message must also be converted to TF-IDF.\n",
        "    # Use the SAME tfidf_vectorizer that was fitted on the training data.\n",
        "    # The vectorizer expects a list of strings, so put the single message in a list.\n",
        "    pred_text_tfidf = tfidf_vectorizer.transform([pred_text])\n",
        "\n",
        "    # --- Step 2: Use the trained model to predict ---\n",
        "    # model.predict() returns the model's output. For a sigmoid output layer, this is the probability.\n",
        "    # The output shape will be (number_of_messages, 1), so we need to access the value.\n",
        "    prediction_probability = model.predict(pred_text_tfidf)[0][0]\n",
        "\n",
        "    # --- Step 3: Determine the class label based on the probability ---\n",
        "    # If the probability of the positive class (spam, assuming it's encoded as 1) is >= 0.5, classify as spam.\n",
        "    # Otherwise, classify as ham.\n",
        "    # We can use the label_encoder to map the predicted class (0 or 1) back to the string label ('ham' or 'spam').\n",
        "\n",
        "    # Determine the predicted class (0 or 1)\n",
        "    predicted_class = 1 if prediction_probability >= 0.5 else 0\n",
        "\n",
        "    # Map the predicted class back to the original string label ('ham' or 'spam')\n",
        "    predicted_label = label_encoder.inverse_transform([predicted_class])[0]\n",
        "\n",
        "\n",
        "    # --- Step 4: Format the output as required ---\n",
        "    # The requirement is a list: [probability, label]\n",
        "    prediction_output = [prediction_probability, predicted_label]\n",
        "\n",
        "    return prediction_output\n",
        "\n",
        "# --- Example usage (Optional - you can remove this after pasting into your notebook) ---\n",
        "# pred_text = \"how are you doing today?\"\n",
        "# prediction = predict_message(pred_text)\n",
        "# print(prediction)\n",
        "\n",
        "# pred_text_spam = \"sale today! call now for prize\"\n",
        "# prediction_spam = predict_message(pred_text_spam)\n",
        "# print(prediction_spam)\n"
      ],
      "metadata": {
        "id": "G5G3nUp30R0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dxotov85SjsC"
      },
      "outputs": [],
      "source": [
        "# Run this cell to test your function and model. Do not modify contents.\n",
        "def test_predictions():\n",
        "  test_messages = [\"how are you doing today\",\n",
        "                   \"sale today! to stop texts call 98912460324\",\n",
        "                   \"i dont want to go. can we try it a different day? available sat\",\n",
        "                   \"our new mobile video service is live. just install on your phone to start watching.\",\n",
        "                   \"you have won Â£1000 cash! call to claim your prize.\",\n",
        "                   \"i'll bring it tomorrow. don't forget the milk.\",\n",
        "                   \"wow, is your arm alright. that happened to me one time too\"\n",
        "                  ]\n",
        "\n",
        "  test_answers = [\"ham\", \"spam\", \"ham\", \"spam\", \"spam\", \"ham\", \"ham\"]\n",
        "  passed = True\n",
        "\n",
        "  for msg, ans in zip(test_messages, test_answers):\n",
        "    prediction = predict_message(msg)\n",
        "    if prediction[1] != ans:\n",
        "      passed = False\n",
        "\n",
        "  if passed:\n",
        "    print(\"You passed the challenge. Great job!\")\n",
        "  else:\n",
        "    print(\"You haven't passed yet. Keep trying.\")\n",
        "\n",
        "test_predictions()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "fcc_sms_text_classification.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {}
  },
  "nbformat": 4,
  "nbformat_minor": 0
}