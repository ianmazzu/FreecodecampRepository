{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rRo8oNqZ-Rj"
      },
      "outputs": [],
      "source": [
        "# Import libraries. You may or may not use all of these.\n",
        "!pip install -q git+https://github.com/tensorflow/docs\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import tensorflow_docs as tfdocs\n",
        "import tensorflow_docs.plots\n",
        "import tensorflow_docs.modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CiX2FI4gZtTt",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Import data\n",
        "!wget https://cdn.freecodecamp.org/project-data/health-costs/insurance.csv\n",
        "dataset = pd.read_csv('insurance.csv')\n",
        "dataset.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LcopvQh3X-kX"
      },
      "outputs": [],
      "source": [
        " #Data Preprocessing\n",
        "\n",
        "# Display the first 5 rows of the dataset to understand its structure\n",
        "print(\"First 5 rows of the dataset:\")\n",
        "print(dataset.head())\n",
        "\n",
        "# Get information about the columns and their data types\n",
        "print(\"\\nDataset Info:\")\n",
        "print(dataset.info())\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\nMissing values per column:\")\n",
        "print(dataset.isnull().sum())\n",
        "\n",
        "# Identify categorical columns that need to be converted to numbers\n",
        "# Based on typical health datasets and the head/info output,\n",
        "# 'sex', 'smoker', and 'region' are likely categorical.\n",
        "\n",
        "# Convert categorical columns to numerical using one-hot encoding\n",
        "# pandas get_dummies is a convenient function for this\n",
        "# It creates new binary columns for each category in the specified columns\n",
        "# drop_first=True is often used to avoid multicollinearity, dropping one category per feature\n",
        "# However, for simple models or when interpretability isn't the main goal, keeping all dummies is fine too.\n",
        "# Let's start by keeping all dummies to represent all categories.\n",
        "# If the model struggles, we could experiment with drop_first=True.\n",
        "\n",
        "print(\"\\nConverting categorical data to numerical using one-hot encoding...\")\n",
        "dataset_processed = pd.get_dummies(dataset, columns=['sex', 'smoker', 'region'])\n",
        "\n",
        "print(\"\\nDataset after one-hot encoding:\")\n",
        "print(dataset_processed.head())\n",
        "print(\"\\nProcessed Dataset Info:\")\n",
        "print(dataset_processed.info())\n",
        "\n",
        "# Now the categorical columns ('sex', 'smoker', 'region') have been replaced\n",
        "# by new numerical (binary) columns like 'sex_female', 'sex_male', etc.\n",
        "# The 'expenses' column is our target variable.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Celda 3\n",
        "# Data Split\n",
        "\n",
        "# Import the necessary function from scikit-learn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Supongamos que el DataFrame procesado del paso anterior se llama dataset_processed\n",
        "\n",
        "# Define the features (X) and the target (y)\n",
        "# Features are all columns EXCEPT 'expenses'\n",
        "# Target is the 'expenses' column\n",
        "X = dataset_processed.drop('expenses', axis=1) # axis=1 means drop a column\n",
        "y = dataset_processed['expenses'] # Select the 'expenses' column\n",
        "\n",
        "print(\"Shape of features (X):\", X.shape)\n",
        "print(\"Shape of target (y):\", y.shape)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "# test_size=0.20 means 20% of the data will be used for testing\n",
        "# train_size=0.80 means 80% of the data will be used for training (this is implicit if test_size is set)\n",
        "# random_state=42 is used to ensure the split is the same every time you run the code.\n",
        "# This makes your results reproducible. You can use any integer for random_state.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
        "\n",
        "# Rename the variables to match the project requirements and the final test cell\n",
        "train_dataset = X_train\n",
        "test_dataset = X_test\n",
        "train_labels = y_train\n",
        "test_labels = y_test\n",
        "\n",
        "print(\"\\nShape of train_dataset:\", train_dataset.shape)\n",
        "print(\"Shape of test_dataset:\", test_dataset.shape)\n",
        "print(\"Shape of train_labels:\", train_labels.shape)\n",
        "print(\"Shape of test_labels:\", test_labels.shape)\n",
        "\n",
        "# Now you have:\n",
        "# train_dataset: Features for training (80% of data)\n",
        "# test_dataset: Features for testing (20% of data)\n",
        "# train_labels: Expenses for training (corresponding to train_dataset)\n",
        "# test_labels: Expenses for testing (corresponding to test_dataset)\n"
      ],
      "metadata": {
        "id": "kAkIUPGFq3dM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Scaling\n",
        "\n",
        "# Import the scaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Identify the numerical columns that need scaling\n",
        "# Based on the dataset info, these are 'age', 'bmi', 'children'.\n",
        "# Note: The one-hot encoded columns ('sex_', 'smoker_', 'region_') are already binary (0/1)\n",
        "# and typically do NOT need scaling with StandardScaler or MinMaxScaler.\n",
        "numerical_cols = ['age', 'bmi', 'children']\n",
        "\n",
        "print(f\"Scaling numerical columns: {numerical_cols}\")\n",
        "\n",
        "# Initialize the scaler\n",
        "# StandardScaler standardizes features by removing the mean and scaling to unit variance.\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler ONLY on the training data and transform the training data\n",
        "# It's CRUCIAL to fit ONLY on training data to prevent data leakage from the test set.\n",
        "train_dataset[numerical_cols] = scaler.fit_transform(train_dataset[numerical_cols])\n",
        "\n",
        "# Transform the test data using the SAME scaler fitted on the training data\n",
        "# We use .transform() here, NOT .fit_transform()\n",
        "test_dataset[numerical_cols] = scaler.transform(test_dataset[numerical_cols])\n",
        "\n",
        "print(\"\\nNumerical features scaled.\")\n",
        "\n",
        "# You can optionally inspect the scaled data\n",
        "# print(\"Scaled train_dataset head:\")\n",
        "# print(train_dataset.head())\n",
        "# print(\"\\nScaled test_dataset head:\")\n",
        "# print(test_dataset.head())\n",
        "\n",
        "# The train_dataset and test_dataset DataFrames now have their numerical columns scaled.\n",
        "# The categorical (one-hot encoded) columns remain unchanged.\n"
      ],
      "metadata": {
        "id": "E9gw8A5ztsIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Definition and Compilation\n",
        "\n",
        "# Import necessary layers if not already imported (they should be from previous imports)\n",
        "# from tensorflow.keras.models import Sequential\n",
        "# from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Define the model architecture\n",
        "# We use a Sequential model, which is a linear stack of layers.\n",
        "model = keras.Sequential([\n",
        "    # The first layer needs to know the input shape.\n",
        "    # The input shape is the number of features in our dataset (all columns except 'expenses').\n",
        "    # We can get this from the shape of train_dataset.\n",
        "    layers.Dense(64, activation='relu', input_shape=[len(train_dataset.keys())]),\n",
        "    # Add more Dense layers to allow the model to learn complex relationships.\n",
        "    # The number of units (e.g., 64, 32) and layers are hyperparameters you could tune.\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    # The output layer for a regression model has a single unit.\n",
        "    # We don't typically use an activation function on the output layer for regression,\n",
        "    # or sometimes a linear activation (which is the default if no activation is specified).\n",
        "    layers.Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "# We need to specify an optimizer, a loss function, and metrics to evaluate during training.\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001) # Adam is a popular optimizer. learning_rate is a hyperparameter.\n",
        "\n",
        "# Loss function: Measures how far the model's predictions are from the true labels.\n",
        "# For regression, Mean Squared Error (MSE) or Mean Absolute Error (MAE) are common.\n",
        "# MSE penalizes larger errors more. MAE is less sensitive to outliers and is the project's evaluation metric.\n",
        "loss_function = 'mse' # Use Mean Squared Error as the primary loss for optimization\n",
        "\n",
        "# Metrics: What we want to monitor during training and evaluation.\n",
        "# The project requires evaluating based on Mean Absolute Error (MAE).\n",
        "metrics_to_monitor = ['mae', 'mse'] # Monitor both MAE and MSE\n",
        "\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=loss_function,\n",
        "              metrics=metrics_to_monitor)\n",
        "\n",
        "# Print the model summary to see the layers and parameter counts\n",
        "model.summary()\n",
        "\n",
        "# The model is now defined and compiled, ready for training.\n"
      ],
      "metadata": {
        "id": "UWqtFEGbrmIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Training\n",
        "\n",
        "# Train the model\n",
        "# We use the .fit() method to start the training process.\n",
        "# train_dataset: The features for training.\n",
        "# train_labels: The true values (expenses) for the training data.\n",
        "# epochs: The number of times the model will iterate over the entire training dataset.\n",
        "#         A higher number of epochs can lead to better learning, but also to overfitting.\n",
        "#         You might need to experiment with the number of epochs. Let's start with 100.\n",
        "# verbose: Controls how much output is shown during training.\n",
        "#          0 = silent, 1 = progress bar, 2 = one line per epoch. Let's use 1 for progress.\n",
        "# You could also add validation_split or validation_data here to monitor performance\n",
        "# on a separate validation set during training, but the project primarily evaluates\n",
        "# on the final test_dataset.\n",
        "\n",
        "print(\"Starting model training...\")\n",
        "\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    train_labels,\n",
        "    epochs=200, # You can adjust the number of epochs\n",
        "    verbose=1 # Set to 0 for silent training, 2 for more detailed output\n",
        ")\n",
        "\n",
        "print(\"Model training finished.\")\n",
        "\n",
        "# The history object contains the training loss and metrics for each epoch.\n",
        "# You could potentially use this to plot training progress later if desired,\n",
        "# but the project's final evaluation uses the test set directly.\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "3Ygb0etfsVgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xe7RXH3N3CWU"
      },
      "outputs": [],
      "source": [
        "# RUN THIS CELL TO TEST YOUR MODEL. DO NOT MODIFY CONTENTS.\n",
        "# Test model by checking how well the model generalizes using the test set.\n",
        "loss, mae, mse = model.evaluate(test_dataset, test_labels, verbose=2)\n",
        "\n",
        "print(\"Testing set Mean Abs Error: {:5.2f} expenses\".format(mae))\n",
        "\n",
        "if mae < 3500:\n",
        "  print(\"You passed the challenge. Great job!\")\n",
        "else:\n",
        "  print(\"The Mean Abs Error must be less than 3500. Keep trying.\")\n",
        "\n",
        "# Plot predictions.\n",
        "test_predictions = model.predict(test_dataset).flatten()\n",
        "\n",
        "a = plt.axes(aspect='equal')\n",
        "plt.scatter(test_labels, test_predictions)\n",
        "plt.xlabel('True values (expenses)')\n",
        "plt.ylabel('Predictions (expenses)')\n",
        "lims = [0, 50000]\n",
        "plt.xlim(lims)\n",
        "plt.ylim(lims)\n",
        "_ = plt.plot(lims,lims)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "fcc_predict_health_costs_with_regression.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}